{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a6ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a135e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369a0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb9b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('GPU')) # Empty # This is Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015169f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.6.0-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 114.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (57.4.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (7.1.2)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 104.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 3.6 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Building wheels for collected packages: promise, pathtools\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=bf666d7cdd7498f075ccba2f968bc4bce17396553e13b59e9ce02850919ca269\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=0133f0ac8d8b628755da8c3b0ce3c31b261bd7841db123cdfdaa48a2ca1b763f\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built promise pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, promise, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.6.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.21\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce69209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca33e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#721fdeb49f2aeea9c5eddef6eea98f3a1ce586d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1353fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseankoval\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/w251/wandb/run-20220709_172414-1jbpwpko</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/seankoval/w251_hw9/runs/1jbpwpko\" target=\"_blank\">dandy-river-27</a></strong> to <a href=\"https://wandb.ai/seankoval/w251_hw9\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/seankoval/w251_hw9/runs/1jbpwpko?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3e7c5fea30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"w251_hw9\", entity=\"seankoval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b862b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586b4649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b78ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b834755",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = 'resnet18'\n",
    "EPOCHS = 2\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PRINT_FREQ = 10\n",
    "TRAIN_BATCH=500\n",
    "VAL_BATCH=500\n",
    "WORKERS=2\n",
    "TRAINDIR=\"/data/train\"\n",
    "VALDIR=\"/data/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82e305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print('GPU not detected.. did you pass through your GPU?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b1b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(config={\"epochs\": EPOCHS, \"batch_size\": TRAIN_BATCH, \"momentum\": MOMENTUM, \"WEIGHT_DECAY\": WEIGHT_DECAY, \"arch\": ARCH})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55fee76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce082d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e23d2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88ac4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    global global_step    \n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    # Grad Scaler\n",
    "    scaler = GradScaler()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if GPU is not None:\n",
    "            images = images.cuda(GPU, non_blocking=True)\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda(GPU, non_blocking=True)\n",
    "\n",
    "        # compute output\n",
    "        with autocast():\n",
    "          output = model(images)\n",
    "          loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # use the scaler\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", loss, global_step = global_step)\n",
    "        writer.add_scalar(\"acc1/train\", top1.avg, global_step = global_step)\n",
    "        writer.add_scalar(\"acc5/train\", top5.avg, global_step = global_step)\n",
    "        \n",
    "        wandb.log({\"Loss/train\": loss, 'acc1/train': top1.avg, 'acc5/train': top5.avg})\n",
    "        \n",
    "        global_step = global_step + 1\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9e6f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    global global_step    \n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            if GPU is not None:\n",
    "                images = images.cuda(GPU, non_blocking=True)\n",
    "            if torch.cuda.is_available():\n",
    "                target = target.cuda(GPU, non_blocking=True)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "    writer.add_scalar(\"Loss/val\", losses.avg, global_step = global_step)\n",
    "    writer.add_scalar(\"acc1/val\", top1.avg, global_step = global_step)\n",
    "    writer.add_scalar(\"acc5/val\", top5.avg, global_step = global_step)    \n",
    "    \n",
    "    wandb.log({\"Loss/val\": losses.avg, 'acc1/val': top1.avg, 'acc5/val': top5.avg})\n",
    "    \n",
    "    global_step = global_step + 1\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "459f0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f3852d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b201083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afb0549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04737d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de65630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "054afdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ddebafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = 2\n",
    "BACKEND = 'nccl'\n",
    "# tcp://172.31.17.123:443\n",
    "URL = 'tcp://54.173.7.235:1883'\n",
    "\n",
    "RANK = 0\n",
    "\n",
    "dist.init_process_group(backend=BACKEND, init_method=URL, rank=RANK, world_size=WORLD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50a2e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "imagenet_std_RGB = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3d44082",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=imagenet_mean_RGB, std=imagenet_std_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12a64871",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be742c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__[ARCH]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a310ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c4c2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(inf, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d13676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a6b69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-172-31-26-243:412:412 [0] NCCL INFO Bootstrap : Using ens5:172.31.26.243<0>\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO Plugin Path : /opt/hpcx/nccl_rdma_sharp_plugin/lib/libnccl-net.so\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO P2P plugin IBext\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO NET/IB : No device found.\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO NET/IB : No device found.\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.26.243<0>\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.10.3+cuda11.4\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Channel 00/02 :    0   1\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Channel 01/02 :    0   1\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Channel 00 : 1[1e0] -> 0[1e0] [receive] via NET/Socket/0\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Channel 01 : 1[1e0] -> 0[1e0] [receive] via NET/Socket/0\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO NET/Socket: Using 2 threads and 8 sockets per thread\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Channel 00 : 0[1e0] -> 1[1e0] [send] via NET/Socket/0\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Channel 01 : 0[1e0] -> 1[1e0] [send] via NET/Socket/0\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Connected all rings\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO Connected all trees\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\n",
      "ip-172-31-26-243:412:524 [0] NCCL INFO comm 0x7f3d9c008fb0 rank 0 nranks 2 cudaDev 0 busId 1e0 - Init COMPLETE\n",
      "ip-172-31-26-243:412:412 [0] NCCL INFO Launch mode Parallel\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[GPU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41a994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df83fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), LR,\n",
    "                                momentum=MOMENTUM,\n",
    "                                weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f11c853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e703854",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop(IMG_SIZE, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean_RGB, imagenet_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8e1022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    TRAINDIR, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4865ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop(IMG_SIZE, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean_RGB, imagenet_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "750d7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = datasets.ImageFolder(\n",
    "    VALDIR, transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12e74d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=TRAIN_BATCH, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True, sampler=torch.utils.data.distributed.DistributedSampler(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b8d2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=VAL_BATCH, shuffle=False,\n",
    "        num_workers=WORKERS, pin_memory=True, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e00dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "072a998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][   0/1282]\tTime 15.057 (15.057)\tData  3.466 ( 3.466)\tLoss 7.0052e+00 (7.0052e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   1.00 (  1.00)\n",
      "Epoch: [0][  10/1282]\tTime  2.463 ( 2.849)\tData  0.003 ( 0.450)\tLoss 6.9297e+00 (6.9718e+00)\tAcc@1   0.40 (  0.11)\tAcc@5   1.00 (  0.65)\n",
      "Epoch: [0][  20/1282]\tTime  2.335 ( 2.367)\tData  0.003 ( 0.251)\tLoss 6.9399e+00 (6.9599e+00)\tAcc@1   0.20 (  0.14)\tAcc@5   1.80 (  0.90)\n",
      "Epoch: [0][  30/1282]\tTime  3.857 ( 2.250)\tData  0.003 ( 0.183)\tLoss 6.8102e+00 (6.9268e+00)\tAcc@1   0.40 (  0.23)\tAcc@5   1.00 (  1.12)\n",
      "Epoch: [0][  40/1282]\tTime  1.983 ( 2.188)\tData  0.003 ( 0.139)\tLoss 6.7301e+00 (6.8912e+00)\tAcc@1   1.00 (  0.33)\tAcc@5   3.60 (  1.39)\n",
      "Epoch: [0][  50/1282]\tTime  2.560 ( 2.197)\tData  0.002 ( 0.118)\tLoss 6.7253e+00 (6.8476e+00)\tAcc@1   0.20 (  0.37)\tAcc@5   1.80 (  1.63)\n",
      "Epoch: [0][  60/1282]\tTime  2.810 ( 2.133)\tData  0.002 ( 0.099)\tLoss 6.5416e+00 (6.8051e+00)\tAcc@1   1.60 (  0.47)\tAcc@5   3.40 (  1.88)\n",
      "Epoch: [0][  70/1282]\tTime  3.270 ( 2.089)\tData  0.003 ( 0.092)\tLoss 6.4297e+00 (6.7578e+00)\tAcc@1   1.40 (  0.59)\tAcc@5   3.80 (  2.23)\n",
      "Epoch: [0][  80/1282]\tTime  2.756 ( 2.059)\tData  0.002 ( 0.085)\tLoss 6.3948e+00 (6.7162e+00)\tAcc@1   1.00 (  0.68)\tAcc@5   4.20 (  2.48)\n",
      "Epoch: [0][  90/1282]\tTime  2.840 ( 2.028)\tData  0.003 ( 0.084)\tLoss 6.4167e+00 (6.6761e+00)\tAcc@1   1.00 (  0.76)\tAcc@5   5.00 (  2.71)\n",
      "Epoch: [0][ 100/1282]\tTime  2.319 ( 2.014)\tData  0.104 ( 0.086)\tLoss 6.1712e+00 (6.6314e+00)\tAcc@1   2.60 (  0.85)\tAcc@5   7.00 (  3.05)\n",
      "Epoch: [0][ 110/1282]\tTime  2.409 ( 1.995)\tData  0.002 ( 0.114)\tLoss 6.1318e+00 (6.5886e+00)\tAcc@1   1.80 (  0.91)\tAcc@5   9.60 (  3.39)\n",
      "Epoch: [0][ 120/1282]\tTime  2.658 ( 1.980)\tData  0.002 ( 0.117)\tLoss 6.2115e+00 (6.5494e+00)\tAcc@1   2.20 (  1.01)\tAcc@5   6.80 (  3.71)\n",
      "Epoch: [0][ 130/1282]\tTime  2.756 ( 1.977)\tData  0.003 ( 0.113)\tLoss 6.0328e+00 (6.5144e+00)\tAcc@1   2.80 (  1.12)\tAcc@5   7.80 (  4.00)\n",
      "Epoch: [0][ 140/1282]\tTime  3.405 ( 1.967)\tData  0.003 ( 0.110)\tLoss 5.8812e+00 (6.4786e+00)\tAcc@1   2.40 (  1.20)\tAcc@5   9.00 (  4.28)\n",
      "Epoch: [0][ 150/1282]\tTime  1.948 ( 1.957)\tData  0.003 ( 0.117)\tLoss 5.8477e+00 (6.4397e+00)\tAcc@1   3.80 (  1.32)\tAcc@5   9.20 (  4.58)\n",
      "Epoch: [0][ 160/1282]\tTime  2.532 ( 1.954)\tData  0.002 ( 0.151)\tLoss 5.8904e+00 (6.4043e+00)\tAcc@1   1.20 (  1.40)\tAcc@5   6.40 (  4.85)\n",
      "Epoch: [0][ 170/1282]\tTime  0.686 ( 1.944)\tData  0.003 ( 0.174)\tLoss 5.8155e+00 (6.3702e+00)\tAcc@1   3.20 (  1.49)\tAcc@5   9.80 (  5.16)\n",
      "Epoch: [0][ 180/1282]\tTime  0.685 ( 1.936)\tData  0.002 ( 0.224)\tLoss 5.6955e+00 (6.3370e+00)\tAcc@1   4.40 (  1.63)\tAcc@5  11.00 (  5.50)\n",
      "Epoch: [0][ 190/1282]\tTime  0.783 ( 1.929)\tData  0.002 ( 0.268)\tLoss 5.7754e+00 (6.3076e+00)\tAcc@1   3.60 (  1.74)\tAcc@5  10.40 (  5.79)\n",
      "Epoch: [0][ 200/1282]\tTime  0.777 ( 1.919)\tData  0.003 ( 0.303)\tLoss 5.6095e+00 (6.2757e+00)\tAcc@1   5.40 (  1.85)\tAcc@5  15.20 (  6.14)\n",
      "Epoch: [0][ 210/1282]\tTime  0.688 ( 1.918)\tData  0.002 ( 0.341)\tLoss 5.5693e+00 (6.2463e+00)\tAcc@1   4.60 (  1.96)\tAcc@5  13.40 (  6.45)\n",
      "Epoch: [0][ 220/1282]\tTime  0.689 ( 1.911)\tData  0.002 ( 0.374)\tLoss 5.7086e+00 (6.2172e+00)\tAcc@1   3.80 (  2.05)\tAcc@5  13.20 (  6.73)\n",
      "Epoch: [0][ 230/1282]\tTime  0.688 ( 1.914)\tData  0.002 ( 0.413)\tLoss 5.6449e+00 (6.1906e+00)\tAcc@1   4.00 (  2.16)\tAcc@5  14.20 (  7.02)\n",
      "Epoch: [0][ 240/1282]\tTime  0.784 ( 1.907)\tData  0.098 ( 0.438)\tLoss 5.6153e+00 (6.1642e+00)\tAcc@1   5.20 (  2.27)\tAcc@5  14.00 (  7.30)\n",
      "Epoch: [0][ 250/1282]\tTime  0.825 ( 1.901)\tData  0.102 ( 0.463)\tLoss 5.3388e+00 (6.1354e+00)\tAcc@1   5.80 (  2.40)\tAcc@5  16.80 (  7.65)\n",
      "Epoch: [0][ 260/1282]\tTime  1.528 ( 1.901)\tData  0.093 ( 0.487)\tLoss 5.4603e+00 (6.1085e+00)\tAcc@1   5.40 (  2.51)\tAcc@5  15.80 (  7.95)\n",
      "Epoch: [0][ 270/1282]\tTime  0.915 ( 1.897)\tData  0.002 ( 0.502)\tLoss 5.4601e+00 (6.0819e+00)\tAcc@1   5.40 (  2.61)\tAcc@5  16.20 (  8.28)\n",
      "Epoch: [0][ 280/1282]\tTime  1.341 ( 1.897)\tData  0.002 ( 0.507)\tLoss 5.4664e+00 (6.0558e+00)\tAcc@1   4.40 (  2.72)\tAcc@5  17.00 (  8.62)\n",
      "Epoch: [0][ 290/1282]\tTime  1.119 ( 1.893)\tData  0.094 ( 0.505)\tLoss 5.2901e+00 (6.0306e+00)\tAcc@1   6.40 (  2.83)\tAcc@5  18.00 (  8.93)\n",
      "Epoch: [0][ 300/1282]\tTime  0.890 ( 1.891)\tData  0.095 ( 0.496)\tLoss 5.1533e+00 (6.0060e+00)\tAcc@1   8.60 (  2.95)\tAcc@5  20.80 (  9.22)\n",
      "Epoch: [0][ 310/1282]\tTime  0.787 ( 1.890)\tData  0.095 ( 0.496)\tLoss 5.1590e+00 (5.9815e+00)\tAcc@1   6.40 (  3.07)\tAcc@5  19.00 (  9.52)\n",
      "Epoch: [0][ 320/1282]\tTime  0.793 ( 1.886)\tData  0.099 ( 0.490)\tLoss 5.1366e+00 (5.9582e+00)\tAcc@1   7.20 (  3.19)\tAcc@5  20.40 (  9.84)\n",
      "Epoch: [0][ 330/1282]\tTime  0.935 ( 1.883)\tData  0.240 ( 0.493)\tLoss 5.0382e+00 (5.9352e+00)\tAcc@1   6.60 (  3.31)\tAcc@5  20.00 ( 10.14)\n",
      "Epoch: [0][ 340/1282]\tTime  0.801 ( 1.883)\tData  0.096 ( 0.499)\tLoss 5.2988e+00 (5.9134e+00)\tAcc@1   5.20 (  3.41)\tAcc@5  18.60 ( 10.44)\n",
      "Epoch: [0][ 350/1282]\tTime  0.797 ( 1.880)\tData  0.097 ( 0.499)\tLoss 5.1735e+00 (5.8924e+00)\tAcc@1   6.20 (  3.52)\tAcc@5  20.20 ( 10.72)\n",
      "Epoch: [0][ 360/1282]\tTime  1.150 ( 1.879)\tData  0.450 ( 0.515)\tLoss 5.0976e+00 (5.8714e+00)\tAcc@1   8.40 (  3.64)\tAcc@5  22.20 ( 11.00)\n",
      "Epoch: [0][ 370/1282]\tTime  1.387 ( 1.878)\tData  0.686 ( 0.527)\tLoss 5.0306e+00 (5.8506e+00)\tAcc@1   6.80 (  3.77)\tAcc@5  22.00 ( 11.29)\n",
      "Epoch: [0][ 380/1282]\tTime  1.633 ( 1.878)\tData  0.943 ( 0.544)\tLoss 5.0154e+00 (5.8304e+00)\tAcc@1   8.80 (  3.88)\tAcc@5  22.80 ( 11.57)\n",
      "Epoch: [0][ 390/1282]\tTime  1.565 ( 1.876)\tData  0.874 ( 0.559)\tLoss 5.0783e+00 (5.8115e+00)\tAcc@1   7.60 (  3.98)\tAcc@5  22.60 ( 11.82)\n",
      "Epoch: [0][ 400/1282]\tTime  0.840 ( 1.874)\tData  0.151 ( 0.572)\tLoss 5.0324e+00 (5.7906e+00)\tAcc@1   8.20 (  4.10)\tAcc@5  22.80 ( 12.12)\n",
      "Epoch: [0][ 410/1282]\tTime  1.153 ( 1.876)\tData  0.464 ( 0.588)\tLoss 5.0605e+00 (5.7708e+00)\tAcc@1   8.20 (  4.22)\tAcc@5  22.40 ( 12.38)\n",
      "Epoch: [0][ 420/1282]\tTime  1.145 ( 1.874)\tData  0.457 ( 0.601)\tLoss 4.9497e+00 (5.7513e+00)\tAcc@1   8.40 (  4.34)\tAcc@5  24.20 ( 12.67)\n",
      "Epoch: [0][ 430/1282]\tTime  1.379 ( 1.874)\tData  0.651 ( 0.614)\tLoss 4.9613e+00 (5.7335e+00)\tAcc@1   9.20 (  4.45)\tAcc@5  25.40 ( 12.93)\n",
      "Epoch: [0][ 440/1282]\tTime  1.366 ( 1.873)\tData  0.677 ( 0.626)\tLoss 4.9220e+00 (5.7149e+00)\tAcc@1   9.00 (  4.56)\tAcc@5  22.60 ( 13.18)\n",
      "Epoch: [0][ 450/1282]\tTime  1.503 ( 1.872)\tData  0.810 ( 0.637)\tLoss 4.7514e+00 (5.6955e+00)\tAcc@1  12.60 (  4.69)\tAcc@5  27.80 ( 13.47)\n",
      "Epoch: [0][ 460/1282]\tTime  1.235 ( 1.882)\tData  0.546 ( 0.658)\tLoss 4.8920e+00 (5.6763e+00)\tAcc@1   6.80 (  4.80)\tAcc@5  25.80 ( 13.75)\n",
      "Epoch: [0][ 470/1282]\tTime  1.342 ( 1.880)\tData  0.650 ( 0.668)\tLoss 4.8066e+00 (5.6581e+00)\tAcc@1  10.40 (  4.92)\tAcc@5  25.40 ( 14.02)\n",
      "Epoch: [0][ 480/1282]\tTime  1.694 ( 1.879)\tData  1.003 ( 0.677)\tLoss 4.7083e+00 (5.6398e+00)\tAcc@1  12.00 (  5.05)\tAcc@5  29.40 ( 14.30)\n",
      "Epoch: [0][ 490/1282]\tTime  1.734 ( 1.878)\tData  1.044 ( 0.687)\tLoss 4.7543e+00 (5.6220e+00)\tAcc@1  11.80 (  5.16)\tAcc@5  26.80 ( 14.56)\n",
      "Epoch: [0][ 500/1282]\tTime  1.884 ( 1.879)\tData  1.194 ( 0.697)\tLoss 4.8264e+00 (5.6044e+00)\tAcc@1   9.80 (  5.28)\tAcc@5  25.80 ( 14.81)\n",
      "Epoch: [0][ 510/1282]\tTime  2.101 ( 1.877)\tData  1.403 ( 0.705)\tLoss 4.8093e+00 (5.5872e+00)\tAcc@1  11.80 (  5.40)\tAcc@5  28.60 ( 15.08)\n",
      "Epoch: [0][ 520/1282]\tTime  1.763 ( 1.876)\tData  1.045 ( 0.713)\tLoss 4.7172e+00 (5.5703e+00)\tAcc@1  11.20 (  5.51)\tAcc@5  26.00 ( 15.33)\n",
      "Epoch: [0][ 530/1282]\tTime  1.508 ( 1.874)\tData  0.813 ( 0.720)\tLoss 4.5215e+00 (5.5536e+00)\tAcc@1  11.80 (  5.64)\tAcc@5  32.20 ( 15.60)\n",
      "Epoch: [0][ 540/1282]\tTime  1.952 ( 1.873)\tData  1.259 ( 0.727)\tLoss 4.6886e+00 (5.5379e+00)\tAcc@1  12.00 (  5.75)\tAcc@5  29.60 ( 15.83)\n",
      "Epoch: [0][ 550/1282]\tTime  1.691 ( 1.871)\tData  0.998 ( 0.734)\tLoss 4.7450e+00 (5.5221e+00)\tAcc@1  11.40 (  5.86)\tAcc@5  27.80 ( 16.09)\n",
      "Epoch: [0][ 560/1282]\tTime  2.399 ( 1.871)\tData  1.701 ( 0.741)\tLoss 4.6306e+00 (5.5058e+00)\tAcc@1  11.40 (  5.99)\tAcc@5  29.80 ( 16.34)\n",
      "Epoch: [0][ 570/1282]\tTime  2.101 ( 1.869)\tData  1.410 ( 0.747)\tLoss 4.5122e+00 (5.4898e+00)\tAcc@1  11.20 (  6.11)\tAcc@5  29.00 ( 16.60)\n",
      "Epoch: [0][ 580/1282]\tTime  2.087 ( 1.869)\tData  1.398 ( 0.754)\tLoss 4.5723e+00 (5.4736e+00)\tAcc@1  14.60 (  6.24)\tAcc@5  30.40 ( 16.85)\n",
      "Epoch: [0][ 590/1282]\tTime  2.011 ( 1.867)\tData  1.318 ( 0.760)\tLoss 4.5354e+00 (5.4574e+00)\tAcc@1  13.80 (  6.36)\tAcc@5  31.20 ( 17.11)\n",
      "Epoch: [0][ 600/1282]\tTime  2.039 ( 1.866)\tData  1.343 ( 0.765)\tLoss 4.6270e+00 (5.4424e+00)\tAcc@1  12.60 (  6.47)\tAcc@5  29.20 ( 17.35)\n",
      "Epoch: [0][ 610/1282]\tTime  2.100 ( 1.865)\tData  1.400 ( 0.771)\tLoss 4.5152e+00 (5.4265e+00)\tAcc@1  12.60 (  6.60)\tAcc@5  32.20 ( 17.60)\n",
      "Epoch: [0][ 620/1282]\tTime  4.198 ( 1.870)\tData  3.445 ( 0.782)\tLoss 4.4752e+00 (5.4112e+00)\tAcc@1  14.80 (  6.72)\tAcc@5  35.20 ( 17.87)\n",
      "Epoch: [0][ 630/1282]\tTime  2.284 ( 1.874)\tData  1.586 ( 0.792)\tLoss 4.4434e+00 (5.3957e+00)\tAcc@1  16.40 (  6.84)\tAcc@5  34.80 ( 18.11)\n",
      "Epoch: [0][ 640/1282]\tTime  2.770 ( 1.878)\tData  2.012 ( 0.802)\tLoss 4.5228e+00 (5.3810e+00)\tAcc@1  13.40 (  6.96)\tAcc@5  33.80 ( 18.35)\n",
      "Epoch: [0][ 650/1282]\tTime  2.561 ( 1.877)\tData  1.847 ( 0.806)\tLoss 4.3885e+00 (5.3656e+00)\tAcc@1  13.60 (  7.07)\tAcc@5  34.40 ( 18.59)\n",
      "Epoch: [0][ 660/1282]\tTime  2.611 ( 1.876)\tData  1.866 ( 0.811)\tLoss 4.4582e+00 (5.3508e+00)\tAcc@1  12.40 (  7.19)\tAcc@5  31.60 ( 18.83)\n",
      "Epoch: [0][ 670/1282]\tTime  2.662 ( 1.875)\tData  1.900 ( 0.815)\tLoss 4.4017e+00 (5.3366e+00)\tAcc@1  13.80 (  7.30)\tAcc@5  33.80 ( 19.05)\n",
      "Epoch: [0][ 680/1282]\tTime  2.665 ( 1.874)\tData  1.926 ( 0.819)\tLoss 4.4100e+00 (5.3233e+00)\tAcc@1  16.00 (  7.41)\tAcc@5  33.40 ( 19.26)\n",
      "Epoch: [0][ 690/1282]\tTime  2.642 ( 1.873)\tData  1.908 ( 0.823)\tLoss 4.3148e+00 (5.3089e+00)\tAcc@1  16.20 (  7.52)\tAcc@5  36.40 ( 19.51)\n",
      "Epoch: [0][ 700/1282]\tTime  2.462 ( 1.872)\tData  1.751 ( 0.826)\tLoss 4.3432e+00 (5.2949e+00)\tAcc@1  15.80 (  7.63)\tAcc@5  36.20 ( 19.74)\n",
      "Epoch: [0][ 710/1282]\tTime  2.418 ( 1.871)\tData  1.703 ( 0.830)\tLoss 4.2470e+00 (5.2810e+00)\tAcc@1  20.40 (  7.75)\tAcc@5  37.60 ( 19.96)\n",
      "Epoch: [0][ 720/1282]\tTime  2.687 ( 1.870)\tData  1.926 ( 0.833)\tLoss 4.2297e+00 (5.2667e+00)\tAcc@1  16.00 (  7.87)\tAcc@5  35.40 ( 20.20)\n",
      "Epoch: [0][ 730/1282]\tTime  2.559 ( 1.870)\tData  1.802 ( 0.837)\tLoss 4.2395e+00 (5.2539e+00)\tAcc@1  15.20 (  7.98)\tAcc@5  38.00 ( 20.42)\n",
      "Epoch: [0][ 740/1282]\tTime  2.467 ( 1.869)\tData  1.752 ( 0.840)\tLoss 4.2536e+00 (5.2406e+00)\tAcc@1  16.00 (  8.09)\tAcc@5  36.60 ( 20.65)\n",
      "Epoch: [0][ 750/1282]\tTime  2.747 ( 1.869)\tData  1.974 ( 0.844)\tLoss 4.4289e+00 (5.2284e+00)\tAcc@1  15.40 (  8.18)\tAcc@5  32.40 ( 20.83)\n",
      "Epoch: [0][ 760/1282]\tTime  2.721 ( 1.867)\tData  1.955 ( 0.846)\tLoss 4.2248e+00 (5.2152e+00)\tAcc@1  15.40 (  8.29)\tAcc@5  36.40 ( 21.04)\n",
      "Epoch: [0][ 770/1282]\tTime  2.693 ( 1.867)\tData  1.943 ( 0.850)\tLoss 4.1213e+00 (5.2025e+00)\tAcc@1  19.80 (  8.40)\tAcc@5  38.20 ( 21.25)\n",
      "Epoch: [0][ 780/1282]\tTime  1.719 ( 1.865)\tData  1.004 ( 0.852)\tLoss 4.2671e+00 (5.1899e+00)\tAcc@1  16.20 (  8.51)\tAcc@5  36.40 ( 21.46)\n",
      "Epoch: [0][ 790/1282]\tTime  1.685 ( 1.866)\tData  0.973 ( 0.856)\tLoss 4.3096e+00 (5.1776e+00)\tAcc@1  15.60 (  8.61)\tAcc@5  34.60 ( 21.65)\n",
      "Epoch: [0][ 800/1282]\tTime  2.717 ( 1.867)\tData  1.952 ( 0.860)\tLoss 4.0431e+00 (5.1649e+00)\tAcc@1  19.20 (  8.73)\tAcc@5  37.80 ( 21.86)\n",
      "Epoch: [0][ 810/1282]\tTime  2.357 ( 1.873)\tData  1.641 ( 0.870)\tLoss 4.1685e+00 (5.1523e+00)\tAcc@1  15.40 (  8.84)\tAcc@5  35.80 ( 22.07)\n",
      "Epoch: [0][ 820/1282]\tTime  1.812 ( 1.872)\tData  1.099 ( 0.872)\tLoss 3.9846e+00 (5.1391e+00)\tAcc@1  18.80 (  8.96)\tAcc@5  40.40 ( 22.29)\n",
      "Epoch: [0][ 830/1282]\tTime  1.997 ( 1.871)\tData  1.282 ( 0.875)\tLoss 3.9738e+00 (5.1270e+00)\tAcc@1  18.80 (  9.06)\tAcc@5  42.80 ( 22.50)\n",
      "Epoch: [0][ 840/1282]\tTime  2.046 ( 1.871)\tData  1.331 ( 0.878)\tLoss 4.0765e+00 (5.1145e+00)\tAcc@1  18.40 (  9.18)\tAcc@5  41.40 ( 22.71)\n",
      "Epoch: [0][ 850/1282]\tTime  2.190 ( 1.870)\tData  1.480 ( 0.881)\tLoss 3.8921e+00 (5.1018e+00)\tAcc@1  22.00 (  9.29)\tAcc@5  42.80 ( 22.93)\n",
      "Epoch: [0][ 860/1282]\tTime  1.907 ( 1.869)\tData  1.192 ( 0.883)\tLoss 4.2170e+00 (5.0892e+00)\tAcc@1  14.40 (  9.41)\tAcc@5  36.40 ( 23.14)\n",
      "Epoch: [0][ 870/1282]\tTime  1.691 ( 1.868)\tData  0.975 ( 0.885)\tLoss 4.0621e+00 (5.0770e+00)\tAcc@1  21.80 (  9.52)\tAcc@5  43.00 ( 23.34)\n",
      "Epoch: [0][ 880/1282]\tTime  1.693 ( 1.867)\tData  0.975 ( 0.887)\tLoss 4.1030e+00 (5.0653e+00)\tAcc@1  20.40 (  9.63)\tAcc@5  41.60 ( 23.55)\n",
      "Epoch: [0][ 890/1282]\tTime  2.285 ( 1.867)\tData  1.569 ( 0.890)\tLoss 4.0301e+00 (5.0532e+00)\tAcc@1  21.20 (  9.75)\tAcc@5  40.60 ( 23.76)\n",
      "Epoch: [0][ 900/1282]\tTime  2.325 ( 1.867)\tData  1.611 ( 0.893)\tLoss 3.9833e+00 (5.0416e+00)\tAcc@1  20.00 (  9.87)\tAcc@5  39.60 ( 23.95)\n",
      "Epoch: [0][ 910/1282]\tTime  2.228 ( 1.867)\tData  1.514 ( 0.896)\tLoss 3.9803e+00 (5.0301e+00)\tAcc@1  19.00 (  9.98)\tAcc@5  39.00 ( 24.15)\n",
      "Epoch: [0][ 920/1282]\tTime  2.608 ( 1.867)\tData  1.853 ( 0.899)\tLoss 4.1674e+00 (5.0184e+00)\tAcc@1  15.20 ( 10.09)\tAcc@5  40.00 ( 24.35)\n",
      "Epoch: [0][ 930/1282]\tTime  2.905 ( 1.868)\tData  2.166 ( 0.902)\tLoss 3.9729e+00 (5.0067e+00)\tAcc@1  17.60 ( 10.19)\tAcc@5  42.20 ( 24.55)\n",
      "Epoch: [0][ 940/1282]\tTime  2.716 ( 1.867)\tData  1.961 ( 0.904)\tLoss 3.9593e+00 (4.9956e+00)\tAcc@1  20.40 ( 10.29)\tAcc@5  43.80 ( 24.75)\n",
      "Epoch: [0][ 950/1282]\tTime  3.494 ( 1.868)\tData  2.734 ( 0.907)\tLoss 3.8076e+00 (4.9845e+00)\tAcc@1  24.00 ( 10.40)\tAcc@5  43.80 ( 24.94)\n",
      "Epoch: [0][ 960/1282]\tTime  2.762 ( 1.867)\tData  1.988 ( 0.908)\tLoss 3.9828e+00 (4.9735e+00)\tAcc@1  21.60 ( 10.52)\tAcc@5  42.80 ( 25.13)\n",
      "Epoch: [0][ 970/1282]\tTime  2.643 ( 1.866)\tData  1.886 ( 0.910)\tLoss 4.0352e+00 (4.9628e+00)\tAcc@1  19.80 ( 10.61)\tAcc@5  44.00 ( 25.32)\n",
      "Epoch: [0][ 980/1282]\tTime  2.727 ( 1.866)\tData  1.969 ( 0.912)\tLoss 3.9851e+00 (4.9518e+00)\tAcc@1  18.60 ( 10.72)\tAcc@5  41.00 ( 25.51)\n",
      "Epoch: [0][ 990/1282]\tTime  3.178 ( 1.866)\tData  2.442 ( 0.914)\tLoss 3.8591e+00 (4.9412e+00)\tAcc@1  21.60 ( 10.83)\tAcc@5  44.20 ( 25.70)\n",
      "Epoch: [0][1000/1282]\tTime  3.388 ( 1.866)\tData  2.654 ( 0.916)\tLoss 3.9994e+00 (4.9305e+00)\tAcc@1  21.20 ( 10.94)\tAcc@5  43.60 ( 25.88)\n",
      "Epoch: [0][1010/1282]\tTime  2.766 ( 1.865)\tData  2.005 ( 0.918)\tLoss 3.9016e+00 (4.9197e+00)\tAcc@1  21.20 ( 11.05)\tAcc@5  44.00 ( 26.07)\n",
      "Epoch: [0][1020/1282]\tTime  2.964 ( 1.865)\tData  2.198 ( 0.920)\tLoss 3.8322e+00 (4.9091e+00)\tAcc@1  21.20 ( 11.16)\tAcc@5  44.60 ( 26.26)\n",
      "Epoch: [0][1030/1282]\tTime  2.844 ( 1.866)\tData  2.074 ( 0.922)\tLoss 3.9005e+00 (4.8987e+00)\tAcc@1  21.20 ( 11.27)\tAcc@5  43.40 ( 26.43)\n",
      "Epoch: [0][1040/1282]\tTime  3.024 ( 1.866)\tData  2.275 ( 0.924)\tLoss 3.9975e+00 (4.8881e+00)\tAcc@1  20.60 ( 11.38)\tAcc@5  41.40 ( 26.63)\n",
      "Epoch: [0][1050/1282]\tTime  2.894 ( 1.865)\tData  2.120 ( 0.925)\tLoss 4.0219e+00 (4.8783e+00)\tAcc@1  20.40 ( 11.48)\tAcc@5  41.20 ( 26.80)\n",
      "Epoch: [0][1060/1282]\tTime  2.849 ( 1.865)\tData  2.095 ( 0.927)\tLoss 3.6656e+00 (4.8679e+00)\tAcc@1  25.00 ( 11.59)\tAcc@5  49.40 ( 26.98)\n",
      "Epoch: [0][1070/1282]\tTime  3.149 ( 1.865)\tData  2.391 ( 0.929)\tLoss 3.9358e+00 (4.8580e+00)\tAcc@1  19.20 ( 11.67)\tAcc@5  41.80 ( 27.15)\n",
      "Epoch: [0][1080/1282]\tTime  3.122 ( 1.865)\tData  2.353 ( 0.931)\tLoss 3.7719e+00 (4.8479e+00)\tAcc@1  23.00 ( 11.78)\tAcc@5  47.40 ( 27.33)\n",
      "Epoch: [0][1090/1282]\tTime  2.813 ( 1.864)\tData  2.043 ( 0.932)\tLoss 3.6886e+00 (4.8380e+00)\tAcc@1  25.00 ( 11.88)\tAcc@5  48.00 ( 27.50)\n",
      "Epoch: [0][1100/1282]\tTime  2.561 ( 1.864)\tData  1.845 ( 0.933)\tLoss 3.6789e+00 (4.8280e+00)\tAcc@1  23.40 ( 11.99)\tAcc@5  47.60 ( 27.69)\n",
      "Epoch: [0][1110/1282]\tTime  2.674 ( 1.864)\tData  1.936 ( 0.935)\tLoss 3.8187e+00 (4.8176e+00)\tAcc@1  23.60 ( 12.10)\tAcc@5  46.60 ( 27.88)\n",
      "Epoch: [0][1120/1282]\tTime  2.809 ( 1.863)\tData  2.054 ( 0.936)\tLoss 3.6783e+00 (4.8078e+00)\tAcc@1  23.00 ( 12.20)\tAcc@5  50.40 ( 28.05)\n",
      "Epoch: [0][1130/1282]\tTime  2.022 ( 1.863)\tData  1.309 ( 0.935)\tLoss 3.6165e+00 (4.7980e+00)\tAcc@1  29.40 ( 12.31)\tAcc@5  50.60 ( 28.22)\n",
      "Epoch: [0][1140/1282]\tTime  2.977 ( 1.863)\tData  2.203 ( 0.936)\tLoss 3.7579e+00 (4.7884e+00)\tAcc@1  22.20 ( 12.41)\tAcc@5  45.40 ( 28.39)\n",
      "Epoch: [0][1150/1282]\tTime  3.264 ( 1.864)\tData  2.526 ( 0.938)\tLoss 3.6420e+00 (4.7789e+00)\tAcc@1  21.60 ( 12.49)\tAcc@5  50.00 ( 28.56)\n",
      "Epoch: [0][1160/1282]\tTime  3.147 ( 1.864)\tData  2.395 ( 0.940)\tLoss 3.4849e+00 (4.7687e+00)\tAcc@1  24.60 ( 12.60)\tAcc@5  50.40 ( 28.74)\n",
      "Epoch: [0][1170/1282]\tTime  2.754 ( 1.864)\tData  2.016 ( 0.942)\tLoss 3.5387e+00 (4.7588e+00)\tAcc@1  28.80 ( 12.71)\tAcc@5  50.00 ( 28.91)\n",
      "Epoch: [0][1180/1282]\tTime  1.947 ( 1.863)\tData  1.231 ( 0.943)\tLoss 3.6469e+00 (4.7489e+00)\tAcc@1  24.20 ( 12.80)\tAcc@5  49.00 ( 29.09)\n",
      "Epoch: [0][1190/1282]\tTime  1.417 ( 1.863)\tData  0.705 ( 0.944)\tLoss 3.6989e+00 (4.7391e+00)\tAcc@1  22.80 ( 12.91)\tAcc@5  47.80 ( 29.26)\n",
      "Epoch: [0][1200/1282]\tTime  1.131 ( 1.863)\tData  0.415 ( 0.946)\tLoss 3.4604e+00 (4.7301e+00)\tAcc@1  30.00 ( 13.01)\tAcc@5  52.60 ( 29.43)\n",
      "Epoch: [0][1210/1282]\tTime  0.813 ( 1.864)\tData  0.098 ( 0.949)\tLoss 3.5790e+00 (4.7207e+00)\tAcc@1  26.40 ( 13.11)\tAcc@5  48.60 ( 29.60)\n",
      "Epoch: [0][1220/1282]\tTime  0.811 ( 1.864)\tData  0.098 ( 0.950)\tLoss 3.6497e+00 (4.7114e+00)\tAcc@1  25.80 ( 13.22)\tAcc@5  47.60 ( 29.77)\n",
      "Epoch: [0][1230/1282]\tTime  1.066 ( 1.864)\tData  0.358 ( 0.951)\tLoss 3.5758e+00 (4.7021e+00)\tAcc@1  24.00 ( 13.32)\tAcc@5  49.80 ( 29.94)\n",
      "Epoch: [0][1240/1282]\tTime  0.819 ( 1.864)\tData  0.106 ( 0.952)\tLoss 3.7011e+00 (4.6930e+00)\tAcc@1  24.80 ( 13.42)\tAcc@5  45.80 ( 30.10)\n",
      "Epoch: [0][1250/1282]\tTime  0.728 ( 1.863)\tData  0.003 ( 0.953)\tLoss 3.5771e+00 (4.6839e+00)\tAcc@1  23.60 ( 13.52)\tAcc@5  51.80 ( 30.27)\n",
      "Epoch: [0][1260/1282]\tTime  0.723 ( 1.863)\tData  0.002 ( 0.955)\tLoss 3.3771e+00 (4.6748e+00)\tAcc@1  27.00 ( 13.61)\tAcc@5  53.60 ( 30.43)\n",
      "Epoch: [0][1270/1282]\tTime  0.719 ( 1.863)\tData  0.002 ( 0.956)\tLoss 3.3435e+00 (4.6656e+00)\tAcc@1  29.60 ( 13.71)\tAcc@5  55.80 ( 30.59)\n",
      "Epoch: [0][1280/1282]\tTime  0.718 ( 1.864)\tData  0.002 ( 0.958)\tLoss 3.5877e+00 (4.6565e+00)\tAcc@1  23.00 ( 13.82)\tAcc@5  49.20 ( 30.76)\n",
      "Test: [  0/100]\tTime  7.069 ( 7.069)\tLoss 3.7472e+00 (3.7472e+00)\tAcc@1  23.40 ( 23.40)\tAcc@5  50.80 ( 50.80)\n",
      "Test: [ 10/100]\tTime  3.351 ( 2.196)\tLoss 4.3938e+00 (4.2953e+00)\tAcc@1  23.40 ( 20.20)\tAcc@5  38.80 ( 40.93)\n",
      "Test: [ 20/100]\tTime  3.305 ( 2.065)\tLoss 4.6811e+00 (4.2949e+00)\tAcc@1   7.20 ( 18.77)\tAcc@5  32.80 ( 40.15)\n",
      "Test: [ 30/100]\tTime  3.300 ( 2.005)\tLoss 4.2656e+00 (4.3192e+00)\tAcc@1  17.40 ( 17.41)\tAcc@5  40.80 ( 39.22)\n",
      "Test: [ 40/100]\tTime  3.004 ( 1.983)\tLoss 3.4674e+00 (4.3398e+00)\tAcc@1  28.60 ( 17.86)\tAcc@5  53.20 ( 39.10)\n",
      "Test: [ 50/100]\tTime  3.187 ( 1.960)\tLoss 5.4642e+00 (4.3635e+00)\tAcc@1   6.20 ( 17.25)\tAcc@5  17.60 ( 38.26)\n",
      "Test: [ 60/100]\tTime  3.109 ( 1.948)\tLoss 4.3973e+00 (4.3473e+00)\tAcc@1  18.20 ( 17.50)\tAcc@5  36.00 ( 38.32)\n",
      "Test: [ 70/100]\tTime  3.262 ( 1.929)\tLoss 4.1426e+00 (4.3443e+00)\tAcc@1  15.60 ( 17.34)\tAcc@5  38.80 ( 38.28)\n",
      "Test: [ 80/100]\tTime  3.370 ( 1.917)\tLoss 4.1349e+00 (4.3421e+00)\tAcc@1  22.00 ( 17.42)\tAcc@5  41.80 ( 38.12)\n",
      "Test: [ 90/100]\tTime  3.340 ( 1.907)\tLoss 4.3420e+00 (4.3435e+00)\tAcc@1  18.00 ( 17.27)\tAcc@5  35.40 ( 37.82)\n",
      " * Acc@1 17.348 Acc@5 37.794\n",
      "lr: 0.05\n",
      "Epoch: [1][   0/1282]\tTime  4.744 ( 4.744)\tData  3.982 ( 3.982)\tLoss 3.5760e+00 (3.5760e+00)\tAcc@1  24.40 ( 24.40)\tAcc@5  51.40 ( 51.40)\n",
      "Epoch: [1][  10/1282]\tTime  2.759 ( 2.067)\tData  2.012 ( 1.338)\tLoss 3.6139e+00 (3.5226e+00)\tAcc@1  25.40 ( 27.15)\tAcc@5  50.60 ( 50.87)\n",
      "Epoch: [1][  20/1282]\tTime  2.708 ( 1.928)\tData  1.960 ( 1.196)\tLoss 3.3471e+00 (3.4561e+00)\tAcc@1  30.60 ( 27.86)\tAcc@5  55.80 ( 52.75)\n",
      "Epoch: [1][  30/1282]\tTime  2.818 ( 1.898)\tData  2.069 ( 1.165)\tLoss 3.2909e+00 (3.4308e+00)\tAcc@1  29.00 ( 28.14)\tAcc@5  51.20 ( 53.15)\n",
      "Epoch: [1][  40/1282]\tTime  2.844 ( 1.886)\tData  2.082 ( 1.154)\tLoss 3.3154e+00 (3.4218e+00)\tAcc@1  31.60 ( 28.35)\tAcc@5  57.60 ( 53.34)\n",
      "Epoch: [1][  50/1282]\tTime  2.355 ( 1.865)\tData  1.642 ( 1.135)\tLoss 3.4692e+00 (3.3982e+00)\tAcc@1  28.60 ( 28.77)\tAcc@5  54.00 ( 53.74)\n",
      "Epoch: [1][  60/1282]\tTime  1.841 ( 1.852)\tData  1.127 ( 1.124)\tLoss 3.3918e+00 (3.3815e+00)\tAcc@1  31.20 ( 29.07)\tAcc@5  57.40 ( 54.12)\n",
      "Epoch: [1][  70/1282]\tTime  1.361 ( 1.839)\tData  0.653 ( 1.113)\tLoss 3.3842e+00 (3.3741e+00)\tAcc@1  28.60 ( 29.15)\tAcc@5  57.00 ( 54.36)\n",
      "Epoch: [1][  80/1282]\tTime  0.811 ( 1.835)\tData  0.103 ( 1.110)\tLoss 3.3091e+00 (3.3691e+00)\tAcc@1  32.00 ( 29.22)\tAcc@5  53.60 ( 54.42)\n",
      "Epoch: [1][  90/1282]\tTime  0.878 ( 1.832)\tData  0.165 ( 1.106)\tLoss 3.1632e+00 (3.3614e+00)\tAcc@1  34.60 ( 29.33)\tAcc@5  58.20 ( 54.51)\n",
      "Epoch: [1][ 100/1282]\tTime  0.814 ( 1.826)\tData  0.098 ( 1.099)\tLoss 3.3666e+00 (3.3522e+00)\tAcc@1  32.60 ( 29.48)\tAcc@5  53.60 ( 54.70)\n",
      "Epoch: [1][ 110/1282]\tTime  0.724 ( 1.823)\tData  0.003 ( 1.095)\tLoss 3.1403e+00 (3.3473e+00)\tAcc@1  32.20 ( 29.55)\tAcc@5  58.40 ( 54.76)\n",
      "Epoch: [1][ 120/1282]\tTime  0.810 ( 1.816)\tData  0.095 ( 1.087)\tLoss 3.3931e+00 (3.3409e+00)\tAcc@1  27.80 ( 29.61)\tAcc@5  54.20 ( 54.92)\n",
      "Epoch: [1][ 130/1282]\tTime  1.094 ( 1.815)\tData  0.383 ( 1.087)\tLoss 3.2099e+00 (3.3354e+00)\tAcc@1  33.00 ( 29.67)\tAcc@5  60.00 ( 54.97)\n",
      "Epoch: [1][ 140/1282]\tTime  0.814 ( 1.812)\tData  0.100 ( 1.083)\tLoss 3.1462e+00 (3.3246e+00)\tAcc@1  31.60 ( 29.80)\tAcc@5  59.00 ( 55.14)\n",
      "Epoch: [1][ 150/1282]\tTime  0.721 ( 1.815)\tData  0.002 ( 1.085)\tLoss 3.3041e+00 (3.3168e+00)\tAcc@1  31.40 ( 29.95)\tAcc@5  54.60 ( 55.29)\n",
      "Epoch: [1][ 160/1282]\tTime  0.725 ( 1.818)\tData  0.003 ( 1.088)\tLoss 3.1635e+00 (3.3121e+00)\tAcc@1  30.60 ( 30.00)\tAcc@5  57.20 ( 55.36)\n",
      "Epoch: [1][ 170/1282]\tTime  0.725 ( 1.817)\tData  0.002 ( 1.086)\tLoss 3.3068e+00 (3.3069e+00)\tAcc@1  27.80 ( 30.06)\tAcc@5  53.40 ( 55.44)\n",
      "Epoch: [1][ 180/1282]\tTime  0.724 ( 1.815)\tData  0.003 ( 1.084)\tLoss 3.1756e+00 (3.3022e+00)\tAcc@1  31.60 ( 30.15)\tAcc@5  57.20 ( 55.50)\n",
      "Epoch: [1][ 190/1282]\tTime  1.049 ( 1.823)\tData  0.335 ( 1.091)\tLoss 3.2274e+00 (3.2984e+00)\tAcc@1  31.00 ( 30.19)\tAcc@5  57.00 ( 55.57)\n",
      "Epoch: [1][ 200/1282]\tTime  1.018 ( 1.826)\tData  0.303 ( 1.094)\tLoss 3.1740e+00 (3.2930e+00)\tAcc@1  33.00 ( 30.27)\tAcc@5  58.40 ( 55.70)\n",
      "Epoch: [1][ 210/1282]\tTime  1.253 ( 1.826)\tData  0.538 ( 1.094)\tLoss 3.2831e+00 (3.2887e+00)\tAcc@1  28.80 ( 30.33)\tAcc@5  55.40 ( 55.79)\n",
      "Epoch: [1][ 220/1282]\tTime  1.268 ( 1.823)\tData  0.552 ( 1.093)\tLoss 3.3287e+00 (3.2848e+00)\tAcc@1  27.00 ( 30.38)\tAcc@5  54.40 ( 55.86)\n",
      "Epoch: [1][ 230/1282]\tTime  0.940 ( 1.822)\tData  0.230 ( 1.091)\tLoss 3.2523e+00 (3.2815e+00)\tAcc@1  30.80 ( 30.45)\tAcc@5  58.60 ( 55.92)\n",
      "Epoch: [1][ 240/1282]\tTime  1.227 ( 1.822)\tData  0.513 ( 1.092)\tLoss 3.3120e+00 (3.2782e+00)\tAcc@1  28.80 ( 30.48)\tAcc@5  54.00 ( 55.95)\n",
      "Epoch: [1][ 250/1282]\tTime  1.190 ( 1.822)\tData  0.477 ( 1.092)\tLoss 3.1689e+00 (3.2729e+00)\tAcc@1  29.00 ( 30.53)\tAcc@5  57.20 ( 56.04)\n",
      "Epoch: [1][ 260/1282]\tTime  0.911 ( 1.823)\tData  0.197 ( 1.093)\tLoss 3.0840e+00 (3.2676e+00)\tAcc@1  31.00 ( 30.61)\tAcc@5  59.60 ( 56.13)\n",
      "Epoch: [1][ 270/1282]\tTime  0.811 ( 1.822)\tData  0.099 ( 1.092)\tLoss 3.1828e+00 (3.2640e+00)\tAcc@1  33.60 ( 30.66)\tAcc@5  57.20 ( 56.21)\n",
      "Epoch: [1][ 280/1282]\tTime  0.817 ( 1.820)\tData  0.099 ( 1.090)\tLoss 3.2817e+00 (3.2593e+00)\tAcc@1  28.80 ( 30.75)\tAcc@5  55.60 ( 56.30)\n",
      "Epoch: [1][ 290/1282]\tTime  1.883 ( 1.823)\tData  1.166 ( 1.093)\tLoss 3.2040e+00 (3.2548e+00)\tAcc@1  32.00 ( 30.81)\tAcc@5  57.40 ( 56.38)\n",
      "Epoch: [1][ 300/1282]\tTime  1.287 ( 1.823)\tData  0.570 ( 1.093)\tLoss 3.2672e+00 (3.2523e+00)\tAcc@1  32.40 ( 30.85)\tAcc@5  55.20 ( 56.40)\n",
      "Epoch: [1][ 310/1282]\tTime  1.579 ( 1.824)\tData  0.866 ( 1.095)\tLoss 2.9800e+00 (3.2487e+00)\tAcc@1  35.80 ( 30.91)\tAcc@5  60.80 ( 56.48)\n",
      "Epoch: [1][ 320/1282]\tTime  2.020 ( 1.825)\tData  1.299 ( 1.096)\tLoss 3.0934e+00 (3.2467e+00)\tAcc@1  34.40 ( 30.96)\tAcc@5  61.00 ( 56.50)\n",
      "Epoch: [1][ 330/1282]\tTime  1.627 ( 1.824)\tData  0.913 ( 1.096)\tLoss 3.1238e+00 (3.2440e+00)\tAcc@1  31.40 ( 31.01)\tAcc@5  59.00 ( 56.54)\n",
      "Epoch: [1][ 340/1282]\tTime  1.594 ( 1.824)\tData  0.881 ( 1.096)\tLoss 3.0875e+00 (3.2400e+00)\tAcc@1  31.80 ( 31.07)\tAcc@5  57.20 ( 56.60)\n",
      "Epoch: [1][ 350/1282]\tTime  1.988 ( 1.825)\tData  1.274 ( 1.097)\tLoss 3.1640e+00 (3.2361e+00)\tAcc@1  33.80 ( 31.15)\tAcc@5  59.40 ( 56.68)\n",
      "Epoch: [1][ 360/1282]\tTime  1.661 ( 1.824)\tData  0.944 ( 1.096)\tLoss 3.1592e+00 (3.2331e+00)\tAcc@1  31.80 ( 31.17)\tAcc@5  57.20 ( 56.74)\n",
      "Epoch: [1][ 370/1282]\tTime  1.859 ( 1.824)\tData  1.128 ( 1.097)\tLoss 3.0340e+00 (3.2296e+00)\tAcc@1  35.20 ( 31.25)\tAcc@5  60.60 ( 56.81)\n",
      "Epoch: [1][ 380/1282]\tTime  2.165 ( 1.823)\tData  1.447 ( 1.096)\tLoss 3.1187e+00 (3.2266e+00)\tAcc@1  32.40 ( 31.28)\tAcc@5  62.20 ( 56.88)\n",
      "Epoch: [1][ 390/1282]\tTime  2.180 ( 1.823)\tData  1.468 ( 1.097)\tLoss 3.2622e+00 (3.2249e+00)\tAcc@1  30.60 ( 31.31)\tAcc@5  55.40 ( 56.91)\n",
      "Epoch: [1][ 400/1282]\tTime  1.554 ( 1.823)\tData  0.840 ( 1.097)\tLoss 3.2023e+00 (3.2204e+00)\tAcc@1  31.80 ( 31.36)\tAcc@5  59.20 ( 57.01)\n",
      "Epoch: [1][ 410/1282]\tTime  1.929 ( 1.823)\tData  1.218 ( 1.097)\tLoss 3.2110e+00 (3.2173e+00)\tAcc@1  31.60 ( 31.43)\tAcc@5  56.60 ( 57.08)\n",
      "Epoch: [1][ 420/1282]\tTime  2.037 ( 1.824)\tData  1.327 ( 1.098)\tLoss 3.0480e+00 (3.2136e+00)\tAcc@1  35.20 ( 31.48)\tAcc@5  58.60 ( 57.14)\n",
      "Epoch: [1][ 430/1282]\tTime  2.114 ( 1.823)\tData  1.401 ( 1.098)\tLoss 3.1415e+00 (3.2117e+00)\tAcc@1  33.00 ( 31.51)\tAcc@5  59.60 ( 57.19)\n",
      "Epoch: [1][ 440/1282]\tTime  1.902 ( 1.823)\tData  1.187 ( 1.098)\tLoss 3.1242e+00 (3.2085e+00)\tAcc@1  32.80 ( 31.57)\tAcc@5  60.40 ( 57.26)\n",
      "Epoch: [1][ 450/1282]\tTime  2.192 ( 1.822)\tData  1.477 ( 1.097)\tLoss 3.0736e+00 (3.2060e+00)\tAcc@1  33.00 ( 31.61)\tAcc@5  58.60 ( 57.29)\n",
      "Epoch: [1][ 460/1282]\tTime  1.967 ( 1.821)\tData  1.251 ( 1.097)\tLoss 3.1030e+00 (3.2026e+00)\tAcc@1  31.40 ( 31.65)\tAcc@5  61.20 ( 57.36)\n",
      "Epoch: [1][ 470/1282]\tTime  2.117 ( 1.821)\tData  1.399 ( 1.097)\tLoss 2.9285e+00 (3.1994e+00)\tAcc@1  32.80 ( 31.69)\tAcc@5  61.80 ( 57.41)\n",
      "Epoch: [1][ 480/1282]\tTime  2.384 ( 1.822)\tData  1.672 ( 1.098)\tLoss 3.0610e+00 (3.1962e+00)\tAcc@1  35.00 ( 31.74)\tAcc@5  60.40 ( 57.47)\n",
      "Epoch: [1][ 490/1282]\tTime  2.214 ( 1.822)\tData  1.500 ( 1.098)\tLoss 2.9909e+00 (3.1931e+00)\tAcc@1  34.40 ( 31.79)\tAcc@5  62.40 ( 57.52)\n",
      "Epoch: [1][ 500/1282]\tTime  2.433 ( 1.822)\tData  1.715 ( 1.098)\tLoss 3.2096e+00 (3.1906e+00)\tAcc@1  34.60 ( 31.84)\tAcc@5  55.80 ( 57.57)\n",
      "Epoch: [1][ 510/1282]\tTime  2.633 ( 1.822)\tData  1.887 ( 1.098)\tLoss 3.0429e+00 (3.1880e+00)\tAcc@1  34.00 ( 31.89)\tAcc@5  61.00 ( 57.62)\n",
      "Epoch: [1][ 520/1282]\tTime  2.522 ( 1.822)\tData  1.765 ( 1.098)\tLoss 2.8707e+00 (3.1846e+00)\tAcc@1  37.60 ( 31.95)\tAcc@5  64.20 ( 57.68)\n",
      "Epoch: [1][ 530/1282]\tTime  2.498 ( 1.822)\tData  1.769 ( 1.098)\tLoss 2.9068e+00 (3.1816e+00)\tAcc@1  36.40 ( 31.99)\tAcc@5  62.80 ( 57.73)\n",
      "Epoch: [1][ 540/1282]\tTime  2.737 ( 1.822)\tData  1.997 ( 1.097)\tLoss 3.0357e+00 (3.1792e+00)\tAcc@1  33.60 ( 32.02)\tAcc@5  61.40 ( 57.77)\n",
      "Epoch: [1][ 550/1282]\tTime  1.937 ( 1.821)\tData  1.227 ( 1.094)\tLoss 3.2405e+00 (3.1767e+00)\tAcc@1  32.80 ( 32.07)\tAcc@5  57.80 ( 57.81)\n",
      "Epoch: [1][ 560/1282]\tTime  2.203 ( 1.822)\tData  1.487 ( 1.086)\tLoss 2.9887e+00 (3.1742e+00)\tAcc@1  34.80 ( 32.10)\tAcc@5  62.60 ( 57.85)\n",
      "Epoch: [1][ 570/1282]\tTime  3.506 ( 1.826)\tData  2.737 ( 1.088)\tLoss 2.9665e+00 (3.1723e+00)\tAcc@1  35.40 ( 32.14)\tAcc@5  64.40 ( 57.89)\n",
      "Epoch: [1][ 580/1282]\tTime  2.865 ( 1.826)\tData  2.123 ( 1.088)\tLoss 3.0472e+00 (3.1698e+00)\tAcc@1  35.60 ( 32.18)\tAcc@5  62.40 ( 57.94)\n",
      "Epoch: [1][ 590/1282]\tTime  2.711 ( 1.825)\tData  1.951 ( 1.088)\tLoss 2.9837e+00 (3.1664e+00)\tAcc@1  34.60 ( 32.23)\tAcc@5  59.40 ( 58.01)\n",
      "Epoch: [1][ 600/1282]\tTime  2.890 ( 1.826)\tData  2.124 ( 1.088)\tLoss 3.1910e+00 (3.1636e+00)\tAcc@1  32.40 ( 32.28)\tAcc@5  58.80 ( 58.07)\n",
      "Epoch: [1][ 610/1282]\tTime  2.865 ( 1.825)\tData  2.116 ( 1.087)\tLoss 2.9920e+00 (3.1602e+00)\tAcc@1  35.20 ( 32.34)\tAcc@5  59.00 ( 58.13)\n",
      "Epoch: [1][ 620/1282]\tTime  2.939 ( 1.825)\tData  2.174 ( 1.087)\tLoss 2.9737e+00 (3.1576e+00)\tAcc@1  34.40 ( 32.39)\tAcc@5  62.80 ( 58.16)\n",
      "Epoch: [1][ 630/1282]\tTime  2.821 ( 1.825)\tData  2.072 ( 1.087)\tLoss 3.0135e+00 (3.1549e+00)\tAcc@1  36.80 ( 32.43)\tAcc@5  60.60 ( 58.21)\n",
      "Epoch: [1][ 640/1282]\tTime  2.963 ( 1.825)\tData  2.223 ( 1.087)\tLoss 3.0461e+00 (3.1526e+00)\tAcc@1  35.80 ( 32.47)\tAcc@5  61.40 ( 58.25)\n",
      "Epoch: [1][ 650/1282]\tTime  2.775 ( 1.824)\tData  2.021 ( 1.087)\tLoss 2.8350e+00 (3.1491e+00)\tAcc@1  39.00 ( 32.53)\tAcc@5  63.60 ( 58.31)\n",
      "Epoch: [1][ 660/1282]\tTime  2.682 ( 1.824)\tData  1.926 ( 1.087)\tLoss 3.1349e+00 (3.1461e+00)\tAcc@1  29.40 ( 32.58)\tAcc@5  56.40 ( 58.36)\n",
      "Epoch: [1][ 670/1282]\tTime  3.161 ( 1.824)\tData  2.410 ( 1.087)\tLoss 2.9501e+00 (3.1436e+00)\tAcc@1  36.60 ( 32.61)\tAcc@5  61.80 ( 58.41)\n",
      "Epoch: [1][ 680/1282]\tTime  2.894 ( 1.824)\tData  2.127 ( 1.087)\tLoss 2.9908e+00 (3.1416e+00)\tAcc@1  33.60 ( 32.64)\tAcc@5  59.00 ( 58.44)\n",
      "Epoch: [1][ 690/1282]\tTime  2.695 ( 1.825)\tData  1.956 ( 1.085)\tLoss 2.8247e+00 (3.1384e+00)\tAcc@1  38.60 ( 32.69)\tAcc@5  63.40 ( 58.49)\n",
      "Epoch: [1][ 700/1282]\tTime  2.735 ( 1.824)\tData  1.986 ( 1.085)\tLoss 2.9599e+00 (3.1356e+00)\tAcc@1  34.40 ( 32.73)\tAcc@5  59.20 ( 58.54)\n",
      "Epoch: [1][ 710/1282]\tTime  2.868 ( 1.824)\tData  2.124 ( 1.085)\tLoss 2.9813e+00 (3.1332e+00)\tAcc@1  34.60 ( 32.76)\tAcc@5  60.40 ( 58.58)\n",
      "Epoch: [1][ 720/1282]\tTime  2.851 ( 1.825)\tData  2.111 ( 1.085)\tLoss 2.9319e+00 (3.1301e+00)\tAcc@1  36.60 ( 32.80)\tAcc@5  62.40 ( 58.64)\n",
      "Epoch: [1][ 730/1282]\tTime  2.805 ( 1.824)\tData  2.053 ( 1.084)\tLoss 2.9945e+00 (3.1286e+00)\tAcc@1  33.80 ( 32.83)\tAcc@5  61.80 ( 58.66)\n",
      "Epoch: [1][ 740/1282]\tTime  3.218 ( 1.825)\tData  2.474 ( 1.085)\tLoss 2.9254e+00 (3.1261e+00)\tAcc@1  36.40 ( 32.87)\tAcc@5  63.40 ( 58.71)\n",
      "Epoch: [1][ 750/1282]\tTime  2.851 ( 1.826)\tData  2.090 ( 1.086)\tLoss 3.1837e+00 (3.1248e+00)\tAcc@1  32.20 ( 32.88)\tAcc@5  58.00 ( 58.72)\n",
      "Epoch: [1][ 760/1282]\tTime  2.894 ( 1.826)\tData  2.153 ( 1.086)\tLoss 2.8991e+00 (3.1218e+00)\tAcc@1  36.00 ( 32.94)\tAcc@5  63.20 ( 58.77)\n",
      "Epoch: [1][ 770/1282]\tTime  2.883 ( 1.825)\tData  2.121 ( 1.085)\tLoss 2.9284e+00 (3.1193e+00)\tAcc@1  35.40 ( 32.98)\tAcc@5  62.60 ( 58.82)\n",
      "Epoch: [1][ 780/1282]\tTime  2.877 ( 1.825)\tData  2.104 ( 1.085)\tLoss 2.9534e+00 (3.1167e+00)\tAcc@1  34.40 ( 33.02)\tAcc@5  61.00 ( 58.87)\n",
      "Epoch: [1][ 790/1282]\tTime  2.952 ( 1.825)\tData  2.181 ( 1.084)\tLoss 3.0088e+00 (3.1145e+00)\tAcc@1  34.40 ( 33.05)\tAcc@5  61.80 ( 58.92)\n",
      "Epoch: [1][ 800/1282]\tTime  2.933 ( 1.825)\tData  2.182 ( 1.084)\tLoss 2.8752e+00 (3.1119e+00)\tAcc@1  37.20 ( 33.10)\tAcc@5  60.40 ( 58.95)\n",
      "Epoch: [1][ 810/1282]\tTime  2.965 ( 1.825)\tData  2.224 ( 1.085)\tLoss 2.8715e+00 (3.1097e+00)\tAcc@1  37.00 ( 33.14)\tAcc@5  62.60 ( 59.00)\n",
      "Epoch: [1][ 820/1282]\tTime  2.780 ( 1.825)\tData  2.032 ( 1.085)\tLoss 2.7923e+00 (3.1065e+00)\tAcc@1  37.60 ( 33.19)\tAcc@5  62.20 ( 59.06)\n",
      "Epoch: [1][ 830/1282]\tTime  2.987 ( 1.825)\tData  2.227 ( 1.085)\tLoss 2.7803e+00 (3.1045e+00)\tAcc@1  39.60 ( 33.22)\tAcc@5  65.40 ( 59.09)\n",
      "Epoch: [1][ 840/1282]\tTime  2.809 ( 1.825)\tData  2.038 ( 1.085)\tLoss 2.9620e+00 (3.1020e+00)\tAcc@1  35.20 ( 33.26)\tAcc@5  63.40 ( 59.15)\n",
      "Epoch: [1][ 850/1282]\tTime  2.800 ( 1.825)\tData  2.052 ( 1.085)\tLoss 2.7527e+00 (3.0991e+00)\tAcc@1  37.80 ( 33.32)\tAcc@5  66.80 ( 59.20)\n",
      "Epoch: [1][ 860/1282]\tTime  2.711 ( 1.824)\tData  1.963 ( 1.085)\tLoss 3.0321e+00 (3.0965e+00)\tAcc@1  33.40 ( 33.35)\tAcc@5  58.80 ( 59.24)\n",
      "Epoch: [1][ 870/1282]\tTime  2.546 ( 1.824)\tData  1.788 ( 1.084)\tLoss 2.9214e+00 (3.0942e+00)\tAcc@1  38.60 ( 33.39)\tAcc@5  64.00 ( 59.28)\n",
      "Epoch: [1][ 880/1282]\tTime  2.469 ( 1.825)\tData  1.742 ( 1.085)\tLoss 2.7757e+00 (3.0915e+00)\tAcc@1  38.40 ( 33.43)\tAcc@5  65.40 ( 59.33)\n",
      "Epoch: [1][ 890/1282]\tTime  2.739 ( 1.825)\tData  1.988 ( 1.085)\tLoss 2.9040e+00 (3.0889e+00)\tAcc@1  34.00 ( 33.47)\tAcc@5  62.60 ( 59.37)\n",
      "Epoch: [1][ 900/1282]\tTime  2.730 ( 1.825)\tData  1.956 ( 1.085)\tLoss 2.8668e+00 (3.0865e+00)\tAcc@1  36.40 ( 33.51)\tAcc@5  62.20 ( 59.41)\n",
      "Epoch: [1][ 910/1282]\tTime  2.719 ( 1.824)\tData  1.941 ( 1.085)\tLoss 2.8269e+00 (3.0840e+00)\tAcc@1  38.20 ( 33.56)\tAcc@5  60.80 ( 59.45)\n",
      "Epoch: [1][ 920/1282]\tTime  2.773 ( 1.824)\tData  2.009 ( 1.084)\tLoss 2.9623e+00 (3.0817e+00)\tAcc@1  37.20 ( 33.60)\tAcc@5  62.60 ( 59.49)\n",
      "Epoch: [1][ 930/1282]\tTime  2.945 ( 1.824)\tData  2.179 ( 1.084)\tLoss 2.8544e+00 (3.0790e+00)\tAcc@1  35.00 ( 33.64)\tAcc@5  64.00 ( 59.54)\n",
      "Epoch: [1][ 940/1282]\tTime  2.817 ( 1.823)\tData  2.076 ( 1.084)\tLoss 2.8690e+00 (3.0770e+00)\tAcc@1  36.60 ( 33.68)\tAcc@5  64.00 ( 59.59)\n",
      "Epoch: [1][ 950/1282]\tTime  2.923 ( 1.824)\tData  2.186 ( 1.084)\tLoss 2.7312e+00 (3.0745e+00)\tAcc@1  37.40 ( 33.72)\tAcc@5  64.00 ( 59.64)\n",
      "Epoch: [1][ 960/1282]\tTime  2.881 ( 1.824)\tData  2.115 ( 1.084)\tLoss 2.9830e+00 (3.0723e+00)\tAcc@1  35.20 ( 33.76)\tAcc@5  61.00 ( 59.68)\n",
      "Epoch: [1][ 970/1282]\tTime  2.979 ( 1.824)\tData  2.239 ( 1.085)\tLoss 2.9623e+00 (3.0704e+00)\tAcc@1  36.40 ( 33.77)\tAcc@5  62.80 ( 59.71)\n",
      "Epoch: [1][ 980/1282]\tTime  2.814 ( 1.824)\tData  2.070 ( 1.085)\tLoss 3.0479e+00 (3.0683e+00)\tAcc@1  32.80 ( 33.82)\tAcc@5  60.20 ( 59.75)\n",
      "Epoch: [1][ 990/1282]\tTime  3.164 ( 1.824)\tData  2.410 ( 1.085)\tLoss 2.8479e+00 (3.0663e+00)\tAcc@1  38.60 ( 33.85)\tAcc@5  62.20 ( 59.79)\n",
      "Epoch: [1][1000/1282]\tTime  2.154 ( 1.824)\tData  1.438 ( 1.084)\tLoss 2.9055e+00 (3.0642e+00)\tAcc@1  37.40 ( 33.88)\tAcc@5  62.60 ( 59.82)\n",
      "Epoch: [1][1010/1282]\tTime  2.006 ( 1.824)\tData  1.291 ( 1.085)\tLoss 2.9010e+00 (3.0619e+00)\tAcc@1  38.80 ( 33.92)\tAcc@5  63.40 ( 59.86)\n",
      "Epoch: [1][1020/1282]\tTime  2.687 ( 1.824)\tData  1.918 ( 1.085)\tLoss 2.8086e+00 (3.0596e+00)\tAcc@1  37.00 ( 33.96)\tAcc@5  63.20 ( 59.90)\n",
      "Epoch: [1][1030/1282]\tTime  2.794 ( 1.824)\tData  2.035 ( 1.085)\tLoss 2.7936e+00 (3.0574e+00)\tAcc@1  38.40 ( 34.00)\tAcc@5  65.80 ( 59.94)\n",
      "Epoch: [1][1040/1282]\tTime  3.055 ( 1.825)\tData  2.284 ( 1.086)\tLoss 2.9858e+00 (3.0555e+00)\tAcc@1  35.40 ( 34.04)\tAcc@5  60.60 ( 59.98)\n",
      "Epoch: [1][1050/1282]\tTime  2.500 ( 1.825)\tData  1.744 ( 1.086)\tLoss 2.9762e+00 (3.0535e+00)\tAcc@1  36.00 ( 34.07)\tAcc@5  61.80 ( 60.02)\n",
      "Epoch: [1][1060/1282]\tTime  2.926 ( 1.826)\tData  2.175 ( 1.087)\tLoss 2.6501e+00 (3.0511e+00)\tAcc@1  41.60 ( 34.12)\tAcc@5  66.80 ( 60.06)\n",
      "Epoch: [1][1070/1282]\tTime  3.065 ( 1.826)\tData  2.325 ( 1.087)\tLoss 2.8932e+00 (3.0491e+00)\tAcc@1  33.80 ( 34.14)\tAcc@5  65.60 ( 60.09)\n",
      "Epoch: [1][1080/1282]\tTime  2.902 ( 1.826)\tData  2.131 ( 1.087)\tLoss 2.7474e+00 (3.0471e+00)\tAcc@1  38.20 ( 34.17)\tAcc@5  66.60 ( 60.13)\n",
      "Epoch: [1][1090/1282]\tTime  2.378 ( 1.825)\tData  1.661 ( 1.087)\tLoss 2.7790e+00 (3.0450e+00)\tAcc@1  41.40 ( 34.21)\tAcc@5  64.00 ( 60.17)\n",
      "Epoch: [1][1100/1282]\tTime  2.281 ( 1.825)\tData  1.562 ( 1.087)\tLoss 2.7184e+00 (3.0428e+00)\tAcc@1  40.20 ( 34.26)\tAcc@5  64.60 ( 60.21)\n",
      "Epoch: [1][1110/1282]\tTime  2.665 ( 1.826)\tData  1.301 ( 1.087)\tLoss 2.8796e+00 (3.0406e+00)\tAcc@1  35.40 ( 34.30)\tAcc@5  61.40 ( 60.24)\n",
      "Epoch: [1][1120/1282]\tTime  0.822 ( 1.827)\tData  0.106 ( 1.078)\tLoss 2.7694e+00 (3.0385e+00)\tAcc@1  38.60 ( 34.34)\tAcc@5  66.60 ( 60.27)\n",
      "Epoch: [1][1130/1282]\tTime  0.818 ( 1.826)\tData  0.099 ( 1.069)\tLoss 2.7028e+00 (3.0363e+00)\tAcc@1  40.80 ( 34.37)\tAcc@5  66.00 ( 60.31)\n",
      "Epoch: [1][1140/1282]\tTime  0.806 ( 1.829)\tData  0.099 ( 1.061)\tLoss 2.8339e+00 (3.0341e+00)\tAcc@1  37.00 ( 34.40)\tAcc@5  65.00 ( 60.35)\n",
      "Epoch: [1][1150/1282]\tTime  0.715 ( 1.831)\tData  0.003 ( 1.052)\tLoss 2.7092e+00 (3.0316e+00)\tAcc@1  40.80 ( 34.44)\tAcc@5  67.00 ( 60.39)\n",
      "Epoch: [1][1160/1282]\tTime  0.809 ( 1.830)\tData  0.099 ( 1.044)\tLoss 2.6692e+00 (3.0288e+00)\tAcc@1  42.00 ( 34.48)\tAcc@5  65.40 ( 60.44)\n",
      "Epoch: [1][1170/1282]\tTime  0.824 ( 1.829)\tData  0.111 ( 1.035)\tLoss 2.5461e+00 (3.0260e+00)\tAcc@1  44.00 ( 34.53)\tAcc@5  67.80 ( 60.49)\n",
      "Epoch: [1][1180/1282]\tTime  1.560 ( 1.829)\tData  0.850 ( 1.030)\tLoss 2.9419e+00 (3.0238e+00)\tAcc@1  33.20 ( 34.56)\tAcc@5  65.20 ( 60.53)\n",
      "Epoch: [1][1190/1282]\tTime  1.884 ( 1.828)\tData  1.179 ( 1.029)\tLoss 2.7914e+00 (3.0212e+00)\tAcc@1  39.00 ( 34.60)\tAcc@5  65.80 ( 60.57)\n",
      "Epoch: [1][1200/1282]\tTime  1.424 ( 1.828)\tData  0.711 ( 1.030)\tLoss 2.6143e+00 (3.0192e+00)\tAcc@1  45.00 ( 34.64)\tAcc@5  66.80 ( 60.61)\n",
      "Epoch: [1][1210/1282]\tTime  1.236 ( 1.827)\tData  0.523 ( 1.030)\tLoss 2.8028e+00 (3.0171e+00)\tAcc@1  36.00 ( 34.67)\tAcc@5  64.80 ( 60.64)\n",
      "Epoch: [1][1220/1282]\tTime  1.183 ( 1.827)\tData  0.475 ( 1.031)\tLoss 2.7965e+00 (3.0149e+00)\tAcc@1  40.20 ( 34.71)\tAcc@5  64.80 ( 60.68)\n",
      "Epoch: [1][1230/1282]\tTime  1.599 ( 1.827)\tData  0.892 ( 1.031)\tLoss 2.8345e+00 (3.0129e+00)\tAcc@1  36.00 ( 34.75)\tAcc@5  64.20 ( 60.72)\n",
      "Epoch: [1][1240/1282]\tTime  1.100 ( 1.826)\tData  0.393 ( 1.031)\tLoss 2.8167e+00 (3.0106e+00)\tAcc@1  35.40 ( 34.78)\tAcc@5  65.60 ( 60.76)\n",
      "Epoch: [1][1250/1282]\tTime  0.812 ( 1.826)\tData  0.101 ( 1.032)\tLoss 2.7517e+00 (3.0083e+00)\tAcc@1  41.40 ( 34.82)\tAcc@5  68.20 ( 60.80)\n",
      "Epoch: [1][1260/1282]\tTime  1.034 ( 1.826)\tData  0.324 ( 1.032)\tLoss 2.5654e+00 (3.0063e+00)\tAcc@1  41.60 ( 34.85)\tAcc@5  70.80 ( 60.83)\n",
      "Epoch: [1][1270/1282]\tTime  0.969 ( 1.826)\tData  0.245 ( 1.032)\tLoss 2.5164e+00 (3.0038e+00)\tAcc@1  43.40 ( 34.89)\tAcc@5  70.20 ( 60.87)\n",
      "Epoch: [1][1280/1282]\tTime  1.429 ( 1.826)\tData  0.718 ( 1.033)\tLoss 2.7072e+00 (3.0016e+00)\tAcc@1  40.80 ( 34.93)\tAcc@5  64.20 ( 60.91)\n",
      "Test: [  0/100]\tTime  4.724 ( 4.724)\tLoss 2.3647e+00 (2.3647e+00)\tAcc@1  46.00 ( 46.00)\tAcc@5  75.20 ( 75.20)\n",
      "Test: [ 10/100]\tTime  3.368 ( 2.144)\tLoss 2.6319e+00 (2.9268e+00)\tAcc@1  44.40 ( 37.20)\tAcc@5  68.20 ( 62.89)\n",
      "Test: [ 20/100]\tTime  3.115 ( 2.019)\tLoss 2.9337e+00 (2.9121e+00)\tAcc@1  27.40 ( 34.65)\tAcc@5  62.60 ( 62.67)\n",
      "Test: [ 30/100]\tTime  3.224 ( 1.965)\tLoss 2.9726e+00 (2.8779e+00)\tAcc@1  34.40 ( 33.99)\tAcc@5  61.20 ( 63.48)\n",
      "Test: [ 40/100]\tTime  2.901 ( 1.942)\tLoss 2.8067e+00 (2.8941e+00)\tAcc@1  38.40 ( 34.72)\tAcc@5  66.80 ( 63.25)\n",
      "Test: [ 50/100]\tTime  3.060 ( 1.930)\tLoss 4.2085e+00 (3.0460e+00)\tAcc@1  18.60 ( 32.86)\tAcc@5  39.60 ( 60.59)\n",
      "Test: [ 60/100]\tTime  3.103 ( 1.924)\tLoss 3.5103e+00 (3.0959e+00)\tAcc@1  31.00 ( 32.62)\tAcc@5  50.80 ( 59.63)\n",
      "Test: [ 70/100]\tTime  3.269 ( 1.908)\tLoss 3.7728e+00 (3.1633e+00)\tAcc@1  23.00 ( 31.84)\tAcc@5  49.40 ( 58.50)\n",
      "Test: [ 80/100]\tTime  3.395 ( 1.912)\tLoss 3.3654e+00 (3.2142e+00)\tAcc@1  31.80 ( 31.43)\tAcc@5  53.40 ( 57.60)\n",
      "Test: [ 90/100]\tTime  3.193 ( 1.919)\tLoss 3.5108e+00 (3.2640e+00)\tAcc@1  32.40 ( 30.70)\tAcc@5  51.00 ( 56.76)\n",
      " * Acc@1 31.394 Acc@5 57.460\n",
      "lr: 0.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, 2):\n",
    "#    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('lr: ' + str(scheduler.get_last_lr()[0]))\n",
    "    \n",
    "    writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], global_step = global_step)\n",
    "    \n",
    "    wandb.log({'lr': scheduler.get_last_lr()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46d3234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91b7584a2265b1f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91b7584a2265b1f5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "writer.close()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e78ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
